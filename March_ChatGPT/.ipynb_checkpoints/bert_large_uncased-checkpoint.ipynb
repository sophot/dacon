{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2347aa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /home/sophot/anaconda3/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: tqdm in /home/sophot/anaconda3/lib/python3.9/site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: torch==2.0.0 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torchtext) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/sophot/anaconda3/lib/python3.9/site-packages (from torchtext) (1.21.5)\n",
      "Requirement already satisfied: torchdata==0.6.0 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torchtext) (0.6.0)\n",
      "Requirement already satisfied: requests in /home/sophot/anaconda3/lib/python3.9/site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (8.5.0.96)\n",
      "Requirement already satisfied: networkx in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (2.8.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.10.3.66)\n",
      "Requirement already satisfied: filelock in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (2.11.3)\n",
      "Requirement already satisfied: sympy in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (1.10.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchtext) (11.7.101)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/sophot/anaconda3/lib/python3.9/site-packages (from torchdata==0.6.0->torchtext) (1.26.11)\n",
      "Requirement already satisfied: wheel in /home/sophot/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/sophot/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext) (63.4.1)\n",
      "Requirement already satisfied: cmake in /home/sophot/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchtext) (3.26.1)\n",
      "Requirement already satisfied: lit in /home/sophot/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchtext) (16.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sophot/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sophot/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/sophot/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/sophot/anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchtext) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sophot/anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchtext) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c71a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = get_tokenizer('spacy')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer(text)\n",
    "    \n",
    "    # Convert everything to lowercase\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if not nlp.vocab[t].is_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=preprocess)\n",
    "LABEL = Field(sequential=False, use_vocab=False, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de14cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train_data = TabularDataset(\n",
    "    path='train.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('text', TEXT), ('label', LABEL)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c514364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1478c630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>Israel Parliament to Start Winter Session JERU...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>Two-thirds of business owners say they are pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  label\n",
       "0  TRAIN_00000  Israel Parliament to Start Winter Session JERU...      3\n",
       "1  TRAIN_00001  Two-thirds of business owners say they are pre...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the CSV file into a DataFrame\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Preview the first few rows of the data\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188c3aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a570407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW\n",
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "max_length = 512\n",
    "learning_rate = 1e-5\n",
    "epochs = 3\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.1)\n",
    "# train_data = data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Load the pretrained DistilRoberta tokenizer\n",
    "MODEL_NAME = 'bert-large-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           num_labels=data['label'].nunique(),\n",
    "                                                           output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                           output_hidden_states = False\n",
    "                                                          ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ab89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data and convert it to PyTorch tensors\n",
    "train_encodings = tokenizer(train_data['text'].tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(val_data['text'].tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "train_labels = train_data['label'].tolist()\n",
    "val_labels = val_data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b61dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "train_dataset = MyDataset(train_encodings, train_labels)\n",
    "val_dataset = MyDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251c7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_len):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors='pt',\n",
    "#             truncation=True\n",
    "#         )\n",
    "\n",
    "#         input_ids = encoding['input_ids'].squeeze(0)\n",
    "#         attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': input_ids,\n",
    "#             'attention_mask': attention_mask,\n",
    "#             'label': torch.tensor(label, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "\n",
    "# train_labels = train_data['label'].tolist()\n",
    "# val_labels = val_data['label'].tolist()\n",
    "# # assuming X_train and y_train are your preprocessed data and labels\n",
    "# train_dataset = MyDataset(train_data['text'].tolist(), train_labels, tokenizer, max_len=max_length)\n",
    "# val_dataset = MyDataset(val_data['text'].tolist(), val_labels, tokenizer, max_len=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d85422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the F1 score\n",
    "def compute_f1(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# assuming you have created a transformer model called \"model\" and defined a \"train_dataset\" and a \"eval_dataset\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=epochs,              # total number of training epochs\n",
    "    per_device_train_batch_size=3,  # batch size per device during training\n",
    "    per_device_eval_batch_size=3,   # batch size for evaluation\n",
    "    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
    "    save_total_limit=1,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=learning_rate,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the PyTorch transformer model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset, defined above\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([item['input_ids'] for item in data]),\n",
    "                                'attention_mask': torch.stack([item['attention_mask'] for item in data]),\n",
    "                                'labels': torch.tensor([item['labels'] for item in data])},\n",
    "    compute_metrics=compute_f1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe50dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42660' max='42660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42660/42660 7:11:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.268035</td>\n",
       "      <td>0.924514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>0.260191</td>\n",
       "      <td>0.932608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.309674</td>\n",
       "      <td>0.940714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42660, training_loss=0.2383115242637495, metrics={'train_runtime': 25862.3236, 'train_samples_per_second': 4.948, 'train_steps_per_second': 1.65, 'total_flos': 1.1926819792849306e+17, 'train_loss': 0.2383115242637495, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd81811",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(),}, f'./results/bert_large_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2086bcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./results/bert_large_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714403a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a302e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83334, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the CSV file into a DataFrame\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52fa6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5d0493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 83334/83334 [1:13:40<00:00, 18.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize the true and predicted labels\n",
    "predicted_labels = []\n",
    "\n",
    "# iterate over the test set and make predictions\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(test_data))):\n",
    "        # get the input features and labels\n",
    "        test_encodings = tokenizer(test_data.iloc[idx]['text'], truncation=True, padding=True, max_length=max_length)\n",
    "        \n",
    "        # move the inputs and labels to the device\n",
    "        input_ids = torch.tensor(test_encodings[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(test_encodings[\"attention_mask\"]).to(device)\n",
    "        \n",
    "        # make predictions\n",
    "        outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # append the predicted labels to the lists\n",
    "\n",
    "        predicted_labels.append(predictions.squeeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5993d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data and save to a CSV file\n",
    "submission_df = pd.DataFrame({'id': test_data['id'], 'label': predicted_labels})\n",
    "submission_df.to_csv('submission-bert-large-uncased.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f919a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phot",
   "language": "python",
   "name": "phot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
